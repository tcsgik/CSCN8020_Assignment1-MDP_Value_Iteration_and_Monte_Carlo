{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558894b4",
   "metadata": {},
   "source": [
    "# Problem 1 - Pick-and-Place Robot\n",
    "\n",
    "## Background\n",
    "Design a Markov decision process (MDP) to control a robotic arm to perform repetitive pick-and-place tasks and learn smooth and fast movements.\n",
    "The agent directly controls the motors and receives feedback on the current joint positions and velocities from the state.\n",
    "\n",
    "## Define States, Actions, and Rewards\n",
    "\n",
    "### 1. State (S)\n",
    "States representation current overall posture and motion state of the robotic arm, including each joint’s position and velocity.  \n",
    "\n",
    "The state vector:\n",
    "- Each joint position (angle): $\\theta_1, \\theta_2, ..., \\theta_n$\n",
    "- Each joint velocity (angular speed): $\\dot{\\theta}_1, \\dot{\\theta}_2, ..., \\dot{\\theta}_n$\n",
    "\n",
    "The state is represented as:\n",
    "\n",
    "$s = [\\theta_1, \\theta_2, ..., \\theta_n, \\dot{\\theta}_1, \\dot{\\theta}_2, ..., \\dot{\\theta}_n]$\n",
    "\n",
    "More think about the State, can expand \n",
    "- The end-effector (gripper) position\n",
    "- Object coordinates\n",
    "- A binary flag representing successful grasp/release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73686cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State definition\n",
    "class RobotState:\n",
    "    def __init__(self, joint_angles, joint_velocities):\n",
    "        self.joint_angles = joint_angles  # list or np.array of length n\n",
    "        self.joint_velocities = joint_velocities  # list or np.array of length n\n",
    "        \n",
    "    def as_vector(self):\n",
    "        # Concatenate angles and velocities into a single vector\n",
    "        import numpy as np\n",
    "        return np.concatenate([self.joint_angles, self.joint_velocities])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88762559",
   "metadata": {},
   "source": [
    "### 2. Actions (A)\n",
    "Actions representation motor control instruction for each joint (like torques). Each control value is constrained within the mechanical limits of the hardware to ensure safe operation.\n",
    "\n",
    "The action vector represents the torque applied to each joint:\n",
    "\n",
    "$a = [\\tau_1, \\tau_2, ..., \\tau_n]$\n",
    "\n",
    "Here is a continuous value vector of length n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79f9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions definition\n",
    "class RobotAction:\n",
    "    def __init__(self, torques):\n",
    "        self.torques = torques  # list or np.array of length n\n",
    "        \n",
    "    def clip(self, min_torque, max_torque):\n",
    "        import numpy as np\n",
    "        self.torques = np.clip(self.torques, min_torque, max_torque)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ee907",
   "metadata": {},
   "source": [
    "### 3. Rewards (R)\n",
    "The reward function considers these factors:\n",
    "- Task completion: Large positive reward for completing pick-and-place\n",
    "- Position error: Positive reward for reducing the distance to the target\n",
    "- Smooth movement: Penalty for large or abrupt torques to encourage smooth actions\n",
    "- Time efficiency: Extra reward for finishing faster\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$R(s, a) = R_{\\text{completion}} - \\alpha \\|a\\|^2 - \\beta \\|a - a_{\\text{prev}}\\|^2 - \\gamma \\text{(position error)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865c3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards definition\n",
    "def compute_reward(task_completed, current_action, prev_action, position_error, alpha=0.1, beta=0.1, gamma=1.0):\n",
    "    reward = 0\n",
    "    if task_completed:\n",
    "        reward += 100  # Completion bonus\n",
    "    # Penalty on control effort\n",
    "    reward -= alpha * (current_action**2).sum()\n",
    "    # Penalty for action changes (for smoothness)\n",
    "    reward -= beta * ((current_action - prev_action)**2).sum()\n",
    "    # Penalty for distance to target\n",
    "    reward -= gamma * position_error\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae38af",
   "metadata": {},
   "source": [
    "## MDP Structure Summary\n",
    "- **States (S):** Joint angles and velocities of the arm\n",
    "\n",
    "    $S = \\left\\{ s \\;\\middle|\\; s = [\\theta_1, \\theta_2, ..., \\theta_n, \\dot{\\theta}_1, \\dot{\\theta}_2, ..., \\dot{\\theta}_n] \\right\\}$\n",
    "    \n",
    "- **Actions (A):** Torques applied to each joint (continuous space)\n",
    "\n",
    "    $A = \\left\\{ a \\;\\middle|\\; a = [\\tau_1, \\tau_2, ..., \\tau_n],\\; \\tau_i \\in [\\tau_{\\text{min}}, \\tau_{\\text{max}}] \\right\\}$\n",
    "    \n",
    "- **Transition Probability (P):** Determined by the physics simulation, such as PyBullet or MuJoCo, with the next state depending on the current state and action. Reflecting the deterministic outcome of motor commands and environmental dynamics\n",
    "\n",
    "    $P(s'|s, a)$\n",
    "\n",
    "- **Reward (R):** Encourages task completion, smooth and fast movements\n",
    "\n",
    "    $R(s, a) = R_{\\text{completion}}(s) - \\alpha \\|a\\|^2 - \\beta \\| a - a_{\\text{prev}} \\|^2 - \\gamma \\cdot \\text{(position error)}$\n",
    "\n",
    "- **Discount factor (γ):** Set as needed, such as 0.9 or 0.99, to balance immediate and future rewards\n",
    "\n",
    "    $\\gamma,\\;\\; 0 < \\gamma < 1$\n",
    "\n",
    "- **Objective** is to learn a policy $ \\pi(a|s) $ that maximizes the expected cumulative discounted reward:\n",
    "  \n",
    "  $\\max_\\pi \\;\\; \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\right]$\n",
    "\n",
    "- Termination conditions may be defined as: \n",
    "    \n",
    "    successful placement of the object, collision, or time-out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
