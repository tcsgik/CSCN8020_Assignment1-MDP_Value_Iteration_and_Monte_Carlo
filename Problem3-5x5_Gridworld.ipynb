{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a69497",
   "metadata": {},
   "source": [
    "# Problem 3 - 5x5 Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c677a",
   "metadata": {},
   "source": [
    "## Basic principle\n",
    "\n",
    "Value Iteration calculates the maximum expected cumulative reward for each state, based on the Bellman Optimality Equation:\n",
    "\n",
    "$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) [ R(s,a,s') + \\gamma V^*(s') ]$\n",
    "\n",
    "Define environment:\n",
    "- State $s$ corresponds to each cell coordinate (row, col) in the gridworld.\n",
    "- Actions $a$: right, left, down, and up.\n",
    "- Transitions are deterministic, meaning $P(s'|s,a)$ is either 1 or 0.\n",
    "- Reward $R(s,a,s')$  is the reward obtained when moving to the new state $s'$, defined as +10 for the goal state, -5 for gray cells, and -1 for all other cells.\n",
    "- $gamma$ is the discount factor.\n",
    "\n",
    "\n",
    "## Computation process explanation\n",
    "\n",
    "1. **Initialization**  \n",
    "   Initialize $V(s)$ for all non-terminal states.\n",
    "\n",
    "2. **Iterative Update**  \n",
    "   For each state $s$, evaluate all possible actions $a$, and compute:\n",
    "\n",
    "   $Q(s,a) = R(s,a,s') + \\gamma V(s')$\n",
    "   \n",
    "   where $s'$ is the state reached after taking action $a$.\n",
    "\n",
    "3. **Maximization**  \n",
    "   Update the value function:\n",
    "\n",
    "   $V(s) = \\max_a Q(s,a)$\n",
    "   \n",
    "   while recording the optimal action.\n",
    "\n",
    "4. **Repeat**  \n",
    "   Repeat steps 2-3 until $V$ converges or the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fda205",
   "metadata": {},
   "source": [
    "## Set the lec3_DP Python module path\n",
    "Based on the code in the lec3_DP folder, implement the 5x5 Gridworld MDP and Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65d930da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path setup complete. Ready for imports.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Find lec3_DP path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'playground', 'lec3_DP'))\n",
    "print(\"Path setup complete. Ready for imports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b081a",
   "metadata": {},
   "source": [
    "## Importing GridWorld and Agent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f0bbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from gridworld import GridWorld\n",
    "from value_iteration_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879233aa",
   "metadata": {},
   "source": [
    "## Initialize a 5x5 Gridworld environment and set the reward\n",
    "- End State: (4,4) Reward +10\n",
    "- Gray State: \\{(2,2), (3,0), (0,4)\\} Penalty -5\n",
    "- Other States: Penalty -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ed4c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted reward matrix:\n",
      " [[-1. -1. -1. -1. -5.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -5. -1. -1.]\n",
      " [-5. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a 5x5 Gridworld environment\n",
    "env = GridWorld(5)\n",
    "\n",
    "# Adjust reward: +10 for the end point, -5 for the gray grid, -1 for the rest\n",
    "env.reward = np.ones((env.env_size, env.env_size)) * -1\n",
    "grey_states = [(2, 2), (3, 0), (0, 4)]\n",
    "for gs in grey_states:\n",
    "    env.reward[gs] = -5\n",
    "env.reward[env.terminal_state] = 10\n",
    "\n",
    "print(\"Adjusted reward matrix:\\n\", env.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a1245",
   "metadata": {},
   "source": [
    "## Create an Agent and initialize parameters gamma and theta\n",
    "- Discount factor - $gamma=0.9$\n",
    "- Convergence threshold - $theta=0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "247a284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized. Environment size:  5\n",
      "Initial Value Function:\n",
      " [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, theta_threshold=0.01, gamma=0.9)\n",
    "print(\"Agent initialized. Environment size: \", env.env_size)\n",
    "print(\"Initial Value Function:\\n\", agent.get_value_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c518adf",
   "metadata": {},
   "source": [
    "## Standard Batch Value Iteration\n",
    "Each round uses the old $V$ to calculate the new $V$ and save it to a new array. After the round is completed, the whole thing is copied back to $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a671fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Value Iteration converged after 9 iterations\n",
      "\n",
      "Batch Update Time: 0.0012 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute standard batch value iteration until convergence or the maximum number of iterations is reached\n",
    "MAX_ITER = 1000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for itr in range(MAX_ITER):\n",
    "    new_V = np.copy(agent.get_value_function())\n",
    "    for i in range(env.env_size):\n",
    "        for j in range(env.env_size):\n",
    "            if not env.is_terminal_state(i, j):\n",
    "                new_V[i, j], _, _ = agent.calculate_max_value(i, j)\n",
    "    if agent.is_done(new_V):\n",
    "        print(f\"Batch Value Iteration converged after {itr+1} iterations\")\n",
    "        break\n",
    "    agent.update_value_function(new_V)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nBatch Update Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dd1a0",
   "metadata": {},
   "source": [
    "## Display the best strategies for the optimal state-value function $V∗$ and the optimal policy $π∗$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17efe624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch optimal Value Function:\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Batch optimal Policy:\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right', 'Right|Down', 'Down']\n",
      "['Right', 'Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "agent.update_greedy_policy()\n",
    "print(\"Batch optimal Value Function:\")\n",
    "print(np.round(agent.get_value_function(), 2))\n",
    "print(\"\\nBatch optimal Policy:\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c0b4e",
   "metadata": {},
   "source": [
    "## In-place value iteration\n",
    "The same $V$ array is updated immediately, and the next state calculation uses the latest updated value. Faster convergence is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b3c701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-place Value Iteration converged after 9 iterations\n",
      "\n",
      "In-place Update Time: 0.0016 seconds\n"
     ]
    }
   ],
   "source": [
    "# Reset agent\n",
    "agent = Agent(env, theta_threshold=0.01, gamma=0.9)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Execute in-place value iteration\n",
    "for itr in range(MAX_ITER):\n",
    "    delta = 0\n",
    "    for i in range(env.env_size):\n",
    "        for j in range(env.env_size):\n",
    "            if not env.is_terminal_state(i, j):\n",
    "                v = agent.V[i, j]\n",
    "                new_v, _, _ = agent.calculate_max_value(i, j)\n",
    "                agent.V[i, j] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "    if delta <= agent.theta_threshold:\n",
    "        print(f\"In-place Value Iteration converged after {itr+1} iterations\")\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nIn-place Update Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c797fc",
   "metadata": {},
   "source": [
    "## Display the best strategies for the optimal state-value function $V∗$ and the optimal policy $π∗$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92d81bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place Optimal Value Function:\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "In-place optimal Policy\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right', 'Right|Down', 'Down']\n",
      "['Right', 'Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "# Output in-place results\n",
    "agent.update_greedy_policy()\n",
    "print(\"In-Place Optimal Value Function:\")\n",
    "print(np.round(agent.get_value_function(), 2))\n",
    "print(\"\\nIn-place optimal Policy\")\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90436f69",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- A complete reward setting and state transition strategy are implemented.\n",
    "- Both Batch and In-place value iteration methods converge to the same optimal value function and strategy in this environment.\n",
    "- The In-place method generally converges faster and takes less time, making it a suitable optimization strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
